{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Watch for any changes in model.py, and re-load it automatically.\n",
    "import math\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "from data_loader import get_loader\n",
    "from data_loader_val import get_loader as val_get_loader\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from nlp_utils import clean_sentence, bleu_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "batch_size = 128  # batch size\n",
    "vocab_threshold = 5  # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 256  # dimensionality of image and word embeddings\n",
    "hidden_size = 512  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"training_log.txt\"  # name of file with saved training loss and perplexity\n",
    "# Path to cocoapi dir\n",
    "cocoapi_dir = r\"path/to/cocoapi/dir\"\n",
    "\n",
    "\n",
    "# Amend the image transform below.\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # smaller edge of image resized to 256\n",
    "        transforms.Resize(256),\n",
    "        # get 224x224 crop from random location\n",
    "        transforms.RandomCrop(224),\n",
    "        # horizontally flip image with probability=0.5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # convert the PIL Image to a tensor\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 414113/414113 [01:21<00:00, 5075.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_from_file=vocab_from_file,\n",
    "    cocoapi_loc=cocoapi_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3236\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the training log file.\n",
    "f = open(log_file, \"w\")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i_step in range(1, total_step + 1):\n",
    "\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_saosmpler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        # Passing the inputs through the CNN-RNN model\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculating the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        #         # Uncomment to debug\n",
    "        #         print(outputs.shape, captions.shape)\n",
    "        #         # torch.Size([bs, cap_len, vocab_size]) torch.Size([bs, cap_len])\n",
    "\n",
    "        #         print(outputs.view(-1, vocab_size).shape, captions.view(-1).shape)\n",
    "        #         # torch.Size([bs*cap_len, vocab_size]) torch.Size([bs*cap_len])\n",
    "\n",
    "        # Backwarding pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating the parameters in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Getting training statistics\n",
    "        stats = (\n",
    "            f\"Epoch [{epoch}/{num_epochs}], Step [{i_step}/{total_step}], \"\n",
    "            f\"Loss: {loss.item():.4f}, Perplexity: {np.exp(loss.item()):.4f}\"\n",
    "        )\n",
    "\n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + \"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print(\"\\r\" + stats)\n",
    "\n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(\n",
    "            decoder.state_dict(), os.path.join(\"./models\", \"decoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "        torch.save(\n",
    "            encoder.state_dict(), os.path.join(\"./models\", \"encoder-%d.pkl\" % epoch)\n",
    "        )\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-final.pkl'))\n",
    "torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-final.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(9955, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=9955, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Create the data loader.\n",
    "val_data_loader = val_get_loader(\n",
    "    transform=transform_test, mode=\"valid\", cocoapi_loc=cocoapi_dir\n",
    ")\n",
    "\n",
    "encoder_file = \"encoder-3.pkl\"\n",
    "decoder_file = \"decoder-3.pkl\"\n",
    "\n",
    "# Initialize the encoder and decoder.\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Moving models to GPU if CUDA is available.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Loading the trained weights\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"./models\", encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"./models\", decoder_file)))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda5deb725e5442d8f896cf20ff1189e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# infer captions for all images\n",
    "pred_result = defaultdict(list)\n",
    "for img_id, img in tqdm(val_data_loader):\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img).unsqueeze(1)\n",
    "        output = decoder.sample(features)\n",
    "    sentence = clean_sentence(output, val_data_loader.dataset.vocab.idx2word)\n",
    "    pred_result[img_id.item()].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    os.path.join(cocoapi_dir, \"cocoapi\", \"annotations/captions_val2014.json\"), \"r\"\n",
    ") as f:\n",
    "    caption = json.load(f)\n",
    "\n",
    "valid_annot = caption[\"annotations\"]\n",
    "valid_result = defaultdict(list)\n",
    "for i in valid_annot:\n",
    "    valid_result[i[\"image_id\"]].append(i[\"caption\"].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a bicycle replica with a clock as the front wheel.',\n",
       "  'the bike has a clock as a tire.',\n",
       "  'a black metal bicycle with a clock inside the front wheel.',\n",
       "  'a bicycle figurine in which the front wheel is replaced with a clock\\n',\n",
       "  'a clock with the appearance of the wheel of a bicycle '],\n",
       " ['a black honda motorcycle parked in front of a garage.',\n",
       "  'a honda motorcycle parked in a grass driveway',\n",
       "  'a black honda motorcycle with a dark burgundy seat.',\n",
       "  'ma motorcycle parked on the gravel in front of a garage',\n",
       "  'a motorcycle with its brake extended standing outside'],\n",
       " ['a room with blue walls and a white sink and door.',\n",
       "  'blue and white color scheme in a small bathroom.',\n",
       "  'this is a blue and white bathroom with a wall sink and a lifesaver on the wall.',\n",
       "  'a blue boat themed bathroom with a life preserver on the wall',\n",
       "  'a bathroom with walls that are painted baby blue.']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' a group of horses standing in a field.'],\n",
       " [' a person riding a surf board on a body of water'],\n",
       " [' a woman standing in front of a store window.']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pred_result.values())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2091174140097583"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score(true_sentences=valid_result, predicted_sentences=pred_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad bleu score with only 3 epochs!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
